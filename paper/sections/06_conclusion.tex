\section{Conclusion}

We have demonstrated that larger language models exhibit an inverse scaling law with respect to structural robustness: increased scale correlates with increased vulnerability to metabolic attacks. Our theoretical framework linking Adam optimization dynamics to spectral collapse provides a foundation for understanding this phenomenon.

The discovery that quantization acts as a thermodynamic shield offers both a practical defense mechanism and insight into the underlying attack mechanism. As we continue to scale models, understanding and mitigating structural vulnerabilities will be critical for AI safety.

Our work establishes spectral collapse as a fundamental failure mode in large language models, with implications for model architecture, training procedures, and safety evaluation.
