\section{Conclusion}

We have demonstrated that larger language models exhibit an inverse scaling law with respect to structural robustness: increased scale correlates with increased vulnerability to Hessian-aware eigen-perturbations that exploit null-space structure. Our theoretical framework linking adaptive optimization dynamics to spectral collapse provides a foundation for understanding this phenomenon.

The discovery that quantization acts as a discrete gradient barrier offers both a practical defense mechanism and insight into the underlying attack mechanism. As we continue to scale models, understanding and mitigating structural vulnerabilities will be critical for AI safety.

Our work establishes spectral collapse as a fundamental failure mode in large language models, with implications for model architecture, training procedures, and safety evaluation.
